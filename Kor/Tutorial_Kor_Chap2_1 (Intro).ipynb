{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76JOHigzdcWD"
   },
   "source": [
    "<h1>Tutorial: Fancy Tools for Exploring Data Science with Python</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nLW9TQkjfC4P"
   },
   "source": [
    "*Colab에서 열어보려면 뱃지를 클릭하세요!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/teboozas/python_tutorial_for_data_science/blob/master/Kor/Tutorial_Kor_Chap2_1 (Intro).ipynb\" target=\"_parent\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtJ_5emycKNn"
   },
   "source": [
    "## 2. Python & object-oriented programming(OOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqjJwnscfPAN"
   },
   "source": [
    "### 2.1 Intro: 먼저 코드를 봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujj7acFvCs7d"
   },
   "source": [
    "#### Python으로 작성된 데이터 사이언스/머신러닝 구현 사례\n",
    "아래는 2018년 Google AI에서 발표한 자연언어처리(natural language process; NLP) 모델인 BERT(Bidirectional Encoder Representations from Transformers)의 오픈소스 Python 코드 일부를 발췌한 것입니다. 우선은 모양과 느낌에 집중하며 코드를 '훓어보시길' 권합니다. 숨겨진 셀을 펼쳐보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1j0Dvt4B70k"
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"The main BERT model and related functions.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "class BertModel(object):\n",
    "  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n",
    "\n",
    "  Example usage:\n",
    "\n",
    "  ```python\n",
    "  # Already been converted into WordPiece token ids\n",
    "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
    "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
    "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
    "\n",
    "  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
    "    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
    "\n",
    "  model = modeling.BertModel(config=config, is_training=True,\n",
    "    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "  label_embeddings = tf.get_variable(...)\n",
    "  pooled_output = model.get_pooled_output()\n",
    "  logits = tf.matmul(pooled_output, label_embeddings)\n",
    "  ...\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               config,\n",
    "               is_training,\n",
    "               input_ids,\n",
    "               input_mask=None,\n",
    "               token_type_ids=None,\n",
    "               use_one_hot_embeddings=False,\n",
    "               scope=None):\n",
    "    \"\"\"Constructor for BertModel.\n",
    "\n",
    "    Args:\n",
    "      config: `BertConfig` instance.\n",
    "      is_training: bool. true for training model, false for eval model. Controls\n",
    "        whether dropout will be applied.\n",
    "      input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
    "      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
    "      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n",
    "        embeddings or tf.embedding_lookup() for the word embeddings.\n",
    "      scope: (optional) variable scope. Defaults to \"bert\".\n",
    "\n",
    "    Raises:\n",
    "      ValueError: The config is invalid or one of the input tensor shapes\n",
    "        is invalid.\n",
    "    \"\"\"\n",
    "    config = copy.deepcopy(config)\n",
    "    if not is_training:\n",
    "      config.hidden_dropout_prob = 0.0\n",
    "      config.attention_probs_dropout_prob = 0.0\n",
    "\n",
    "    input_shape = get_shape_list(input_ids, expected_rank=2)\n",
    "    batch_size = input_shape[0]\n",
    "    seq_length = input_shape[1]\n",
    "\n",
    "    if input_mask is None:\n",
    "      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n",
    "\n",
    "    if token_type_ids is None:\n",
    "      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n",
    "\n",
    "    with tf.variable_scope(scope, default_name=\"bert\"):\n",
    "      with tf.variable_scope(\"embeddings\"):\n",
    "        # Perform embedding lookup on the word ids.\n",
    "        (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
    "            input_ids=input_ids,\n",
    "            vocab_size=config.vocab_size,\n",
    "            embedding_size=config.hidden_size,\n",
    "            initializer_range=config.initializer_range,\n",
    "            word_embedding_name=\"word_embeddings\",\n",
    "            use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "        # Add positional embeddings and token type embeddings, then layer\n",
    "        # normalize and perform dropout.\n",
    "        self.embedding_output = embedding_postprocessor(\n",
    "            input_tensor=self.embedding_output,\n",
    "            use_token_type=True,\n",
    "            token_type_ids=token_type_ids,\n",
    "            token_type_vocab_size=config.type_vocab_size,\n",
    "            token_type_embedding_name=\"token_type_embeddings\",\n",
    "            use_position_embeddings=True,\n",
    "            position_embedding_name=\"position_embeddings\",\n",
    "            initializer_range=config.initializer_range,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            dropout_prob=config.hidden_dropout_prob)\n",
    "\n",
    "      with tf.variable_scope(\"encoder\"):\n",
    "        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n",
    "        # mask of shape [batch_size, seq_length, seq_length] which is used\n",
    "        # for the attention scores.\n",
    "        attention_mask = create_attention_mask_from_input_mask(\n",
    "            input_ids, input_mask)\n",
    "\n",
    "        # Run the stacked transformer.\n",
    "        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n",
    "        self.all_encoder_layers = transformer_model(\n",
    "            input_tensor=self.embedding_output,\n",
    "            attention_mask=attention_mask,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_hidden_layers=config.num_hidden_layers,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            intermediate_act_fn=get_activation(config.hidden_act),\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "            initializer_range=config.initializer_range,\n",
    "            do_return_all_layers=True)\n",
    "\n",
    "      self.sequence_output = self.all_encoder_layers[-1]\n",
    "      # The \"pooler\" converts the encoded sequence tensor of shape\n",
    "      # [batch_size, seq_length, hidden_size] to a tensor of shape\n",
    "      # [batch_size, hidden_size]. This is necessary for segment-level\n",
    "      # (or segment-pair-level) classification tasks where we need a fixed\n",
    "      # dimensional representation of the segment.\n",
    "      with tf.variable_scope(\"pooler\"):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token. We assume that this has been pre-trained\n",
    "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
    "        self.pooled_output = tf.layers.dense(\n",
    "            first_token_tensor,\n",
    "            config.hidden_size,\n",
    "            activation=tf.tanh,\n",
    "            kernel_initializer=create_initializer(config.initializer_range))\n",
    "\n",
    "  def get_pooled_output(self):\n",
    "    return self.pooled_output\n",
    "\n",
    "  def get_sequence_output(self):\n",
    "    \"\"\"Gets final hidden layer of encoder.\n",
    "\n",
    "    Returns:\n",
    "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
    "      to the final hidden of the transformer encoder.\n",
    "    \"\"\"\n",
    "    return self.sequence_output\n",
    "\n",
    "  def get_all_encoder_layers(self):\n",
    "    return self.all_encoder_layers\n",
    "\n",
    "  def get_embedding_output(self):\n",
    "    \"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n",
    "\n",
    "    Returns:\n",
    "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
    "      to the output of the embedding layer, after summing the word\n",
    "      embeddings with the positional embeddings and the token type embeddings,\n",
    "      then performing layer normalization. This is the input to the transformer.\n",
    "    \"\"\"\n",
    "    return self.embedding_output\n",
    "\n",
    "  def get_embedding_table(self):\n",
    "    return self.embedding_table\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "  \"\"\"Gaussian Error Linear Unit.\n",
    "\n",
    "  This is a smoother version of the RELU.\n",
    "  Original paper: https://arxiv.org/abs/1606.08415\n",
    "  Args:\n",
    "    x: float Tensor to perform activation.\n",
    "\n",
    "  Returns:\n",
    "    `x` with the GELU activation applied.\n",
    "  \"\"\"\n",
    "  cdf = 0.5 * (1.0 + tf.tanh(\n",
    "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "  return x * cdf\n",
    "\n",
    "\n",
    "def get_activation(activation_string):\n",
    "  \"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n",
    "\n",
    "  Args:\n",
    "    activation_string: String name of the activation function.\n",
    "\n",
    "  Returns:\n",
    "    A Python function corresponding to the activation function. If\n",
    "    `activation_string` is None, empty, or \"linear\", this will return None.\n",
    "    If `activation_string` is not a string, it will return `activation_string`.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: The `activation_string` does not correspond to a known\n",
    "      activation.\n",
    "  \"\"\"\n",
    "\n",
    "  # We assume that anything that\"s not a string is already an activation\n",
    "  # function, so we just return it.\n",
    "  if not isinstance(activation_string, six.string_types):\n",
    "    return activation_string\n",
    "\n",
    "  if not activation_string:\n",
    "    return None\n",
    "\n",
    "  act = activation_string.lower()\n",
    "  if act == \"linear\":\n",
    "    return None\n",
    "  elif act == \"relu\":\n",
    "    return tf.nn.relu\n",
    "  elif act == \"gelu\":\n",
    "    return gelu\n",
    "  elif act == \"tanh\":\n",
    "    return tf.tanh\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported activation: %s\" % act)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbC6RI_hEaJL"
   },
   "source": [
    "#### Python 코드의 특징\n",
    "코드를 보며 다음과 같은 특징을 느끼셨나요?\n",
    "* `from ... import ...`와 `import ... as ...`가 윗 부분에 쌓여있다.\n",
    "* `#`와 `\"\"\" ... \"\"\"`로 뭔가 잔뜩 쓰여있다.\n",
    "* 코드가 정돈된 느낌이 있다.\n",
    "* `class`와 `def`가 코드의 대부분을 차지한다.\n",
    "* `__init__()`와 `self`라는 표현이 자주 눈에 띈다.\n",
    "* 이름에 특정한 패턴이 있다. (`a.b`,  `_c` 등)\n",
    "* 이름과 이름사이는 대부분 `=`로 이어져있다.\n",
    "\n",
    "이 특징들은 사용자나 내용과 무관하게 Python으로 작성된 코드 대부분에게서 나타납니다. 그리고 Python 언어와 Python으로 작성된 프로그래밍 패러다임(paradigm)을 잘 요약하는 것들입니다.\n",
    "\n",
    "우리는 이제부터 이런 특징들이 무엇을 의미하는지, 그 관계는 어떻게 이어져있는지 천천히 살펴보겠습니다. **특히 '객체 지향 프로그래밍'을 중심으로 한 Python 코드의 기본적인 구조를 집중적으로 알아볼 예정입니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Br42bvTMiCXq"
   },
   "source": [
    "### 참고자료 (Python & object-oriented programming(OOP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKvHUd1NiHDw"
   },
   "source": [
    "* [Python 공식 documentation (한국어, 3.6.9버전)](https://docs.python.org/ko/3.6/index.html) - 자습서를 포함한 Python 관련 공식 기술문서를 열람할 수 있습니다.\n",
    "* [점프 투 파이썬 WikiDocs](https://wikidocs.net/book/1) - Python 교재 중 인지도가 높은 책이며, WikiDocs로 무료 배포되어 있습니다.\n",
    "* [a Whirlwind Tour of Python](https://jakevdp.github.io/WhirlwindTourOfPython/) - 명저인 Python data science handbook의 저자가 쓴 Python 입문서입니다.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial_Kor_Chap2_1 (Intro).ipynb",
   "provenance": [
    {
     "file_id": "1puyr3xz0IO5HKFnb4mYNrn4DOYv91v83",
     "timestamp": 1565417897229
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
